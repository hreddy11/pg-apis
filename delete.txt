from google.cloud import bigquery

def export_bigquery_data_to_gcs(project_id, dataset_id, table_id, destination_bucket, destination_blob_name):
    # Initialize BigQuery client
    client = bigquery.Client(project=project_id)

    # Destination URI
    destination_uri = f"gs://{destination_bucket}/{destination_blob_name}"

    # Configure the job
    job_config = bigquery.job.ExtractJobConfig()
    job_config.compression = bigquery.enums.Compression.GZIP
    job_config.destination_format = bigquery.enums.DestinationFormat.CSV

    # SQL query to extract data from the BigQuery table ordered by extract_order column
    query = f"""
        SELECT *
        FROM `{project_id}.{dataset_id}.{table_id}`
        ORDER BY extract_order
    """
    
    # Execute the job
    job = client.extract_table(
        source=query,  # Specify the SQL query as the source
        destination_uris=[destination_uri],
        job_config=job_config,
        location="US",  # Set the location to match your dataset's location
    )

    job.result()  # Waits for the job to complete.

    print(f"Data exported to gs://{destination_bucket}/{destination_blob_name}")

# Replace with your project ID, dataset ID, table ID, destination bucket, and destination blob name
export_bigquery_data_to_gcs('your-project-id', 'your-dataset-id', 'your-table-id', 'your-destination-bucket', 'your-destination-blob-name')
